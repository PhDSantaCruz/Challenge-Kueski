{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "   No scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sys parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"RiskModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7fc886f9dd10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/demo-kueski-p-23edvki0afej/sagemaker-demo-kueski-p-23edvki0afej-modelbuild\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload or / Download the data files to the Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_credit_risk.csv\n",
    "# No scope -> manual for this PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-690391346049/risk/dataset_credit_risk.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/dataset_credit_risk.csv\"\n",
    "\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/risk\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters to Parametrize Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input and bacht data are the same for convenience\n",
    "- Train, test and validation data are the same for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterString(name='InputData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-us-east-2-690391346049/risk/dataset_credit_risk.csv')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Processing Step for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preprocessing.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting risk/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile risk/preprocessing.py\n",
    "\n",
    "from pyspark.sql.functions import dense_rank,col,avg,to_date,round as round_,current_date,to_date as to_date_,months_between,udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import (\n",
    "    #OneHotEncoder,\n",
    "    #StringIndexer,\n",
    "    VectorAssembler,\n",
    "    #VectorIndexer,\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ShortType\n",
    "\n",
    "# Import dataframe into MySQL\n",
    "#import sqlalchemy\n",
    "\n",
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    \n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "\n",
    "\n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "    udf_flag_own_car = udf(lambda x: 0 if x == 'N' else 1,ShortType() )\n",
    "\n",
    "\n",
    "\n",
    "    df_features = spark.read.csv('s3://sagemaker-us-east-2-690391346049/risk/dataset_credit_risk.csv',inferSchema =True, header=True).na.drop()\\\n",
    "    .sort(col('id'),col('loan_date'))\\\n",
    "    .withColumn(\"loan_date\",to_date(col(\"loan_date\"),\"yyyy-MM-dd\"))\\\n",
    "    .withColumn(\"nb_previous_loans\", dense_rank().over(Window.partitionBy(\"id\").orderBy(col(\"loan_date\")))-1)\\\n",
    "    .sort(col('id'),col('loan_date'))\\\n",
    "    .withColumn('avg_amount_loans_previous', avg(col('loan_amount'))\\\n",
    "    .over(Window.partitionBy(col('id')).orderBy(col(\"loan_date\")).rowsBetween(Window.unboundedPreceding,-1)))\\\n",
    "    .withColumn(\"age\",round_(months_between(current_date(),to_date_(col(\"birthday\"), \"yyyy-MM-dd\"), True)/12).cast('int'))\\\n",
    "    .withColumn(\"years_on_the_job\",round_(months_between(current_date(),to_date(col(\"job_start_date\"), \"yyyy-MM-dd\"), True)/12).cast('int'))\\\n",
    "    .withColumn(\"flag_own_car\",udf_flag_own_car(col('flag_own_car')))\\\n",
    "    .select('status', 'age', 'years_on_the_job', 'nb_previous_loans', 'avg_amount_loans_previous', 'flag_own_car')\\\n",
    "    .repartition(1).write.format(\"csv\").option(\"header\", \"true\").save(\"s3://sagemaker-us-east-2-690391346049/risk/train\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "\n",
    "#Def PySpark process\n",
    "pyspark_processor = PySparkProcessor(framework_version='3.0',\n",
    "                                     base_job_name=\"sm-spark\",\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_args = pyspark_processor.get_run_args(\n",
    "    \"risk/preprocessing.py\",\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\")\n",
    "    ],\n",
    "    arguments=None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add spark process into processing Step\n",
    "step_process = ProcessingStep(\n",
    "    name=\"RiskProcess\",\n",
    "    processor=pyspark_processor,\n",
    "    inputs=run_args.inputs,\n",
    "    outputs=run_args.outputs,\n",
    "    job_arguments=run_args.arguments,\n",
    "    code=run_args.code\n",
    ")\n",
    "#/opt/ml/processing/input/dataset_credit_risk.csv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Training Step to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "model_path = f\"s3://{default_bucket}/RiskTrain\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"RiskTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data='s3://sagemaker-us-east-2-690391346049/risk/train_model.csv',\n",
    "            #step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data= 's3://sagemaker-us-east-2-690391346049/risk/train_model.csv',\n",
    "            #step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Model Evaluation Step to Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation script uses xgboost to do the following:\n",
    "\n",
    "- Load the model.\n",
    "\n",
    "- Read the test data.\n",
    "\n",
    "- Issue predictions against the test data.\n",
    "\n",
    "- Build a classification report, including accuracy and ROC curve.\n",
    "\n",
    "- Save the evaluation report to the evaluation directory.\n",
    "\n",
    "\n",
    "After pipeline execution, you can examine the resulting evaluation.json for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting risk/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile risk/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/train/train.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-risk-eval\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"RiskEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput( \n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"risk/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Create Model Step to Create a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create a SageMaker model using the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"RiskCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create a Transformer instance with the appropriate model type, compute instance type, and desired output S3 URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/RiskTransform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"RiskTransform\", transformer=transformer, inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Register Model Step to Create a Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"RiskRegisterModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Condition Step to Check Accuracy and Conditionally Create a Model and Run a Batch Transformation and Register a Model in the Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\",\n",
    "    ),\n",
    "    right=6.0,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"RiskMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline of Parameters, Steps, and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"RiskPipelineSM\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "    ],\n",
    "    steps=[step_process],#, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Examining the pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "#definition = json.loads(pipeline.definition())\n",
    "#definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to SageMaker and start execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "\n",
    "# batch_data,\n",
    "# , step_eval, step_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Operations: Examining and Waiting for Pipeline Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-892d67c8ba32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mwaiter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         )\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPipelineExecutionArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mWaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     wait.__doc__ = WaiterDocstring(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mlast_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_attempts\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_attempts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\""
     ]
    }
   ],
   "source": [
    "execution.wait()\n",
    "\n",
    "execution.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-14 19:42:51      24622 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/output/model.tar.gz\n",
      "2022-02-14 19:42:53          0 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/profiler-output/framework/training_job_end.ts\n",
      "2022-02-14 19:42:49     119051 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/profiler-output/system/incremental/2022021419/1644867660.algo-1.json\n",
      "2022-02-14 19:42:49     193761 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/profiler-output/system/incremental/2022021419/1644867720.algo-1.json\n",
      "2022-02-14 19:42:53          0 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/profiler-output/system/training_job_end.ts\n",
      "2022-02-14 19:44:27     329712 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-report.html\n",
      "2022-02-14 19:44:26     171095 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-report.ipynb\n",
      "2022-02-14 19:44:23        190 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/BatchSize.json\n",
      "2022-02-14 19:44:23        198 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "2022-02-14 19:44:23        126 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/Dataloader.json\n",
      "2022-02-14 19:44:23        127 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "2022-02-14 19:44:23        197 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/IOBottleneck.json\n",
      "2022-02-14 19:44:23        119 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/LoadBalancing.json\n",
      "2022-02-14 19:44:23        151 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "2022-02-14 19:44:23        179 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "2022-02-14 19:44:23        133 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "2022-02-14 19:44:23        464 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "2022-02-14 19:44:23        156 RiskTrain/pipelines-i78mx6pndrp1-RiskTrain-GPdaFBNQ84/rule-output/ProfilerReport-1644867587/profiler-output/profiler-reports/StepOutlier.json\n",
      "2022-02-14 19:54:14      24624 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/output/model.tar.gz\n",
      "2022-02-14 19:54:17          0 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/profiler-output/framework/training_job_end.ts\n",
      "2022-02-14 19:54:11     159518 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/profiler-output/system/incremental/2022021419/1644868380.algo-1.json\n",
      "2022-02-14 19:54:11      44471 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/profiler-output/system/incremental/2022021419/1644868440.algo-1.json\n",
      "2022-02-14 19:54:17          0 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/profiler-output/system/training_job_end.ts\n",
      "2022-02-14 19:55:09     329715 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-report.html\n",
      "2022-02-14 19:55:08     171102 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-report.ipynb\n",
      "2022-02-14 19:55:05        190 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/BatchSize.json\n",
      "2022-02-14 19:55:05        198 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "2022-02-14 19:55:05        126 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/Dataloader.json\n",
      "2022-02-14 19:55:05        127 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "2022-02-14 19:55:05        197 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/IOBottleneck.json\n",
      "2022-02-14 19:55:05        119 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/LoadBalancing.json\n",
      "2022-02-14 19:55:05        151 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "2022-02-14 19:55:05        179 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "2022-02-14 19:55:05        133 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "2022-02-14 19:55:05        474 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "2022-02-14 19:55:05        156 RiskTrain/pipelines-whwvi3nq4hb6-RiskTrain-19ReascNOA/rule-output/ProfilerReport-1644868262/profiler-output/profiler-reports/StepOutlier.json\n",
      "2022-02-14 03:18:18     191873 abalone/abalone-dataset.csv\n",
      "2022-02-14 03:28:54   20375922 abalone/train_model.csv\n",
      "2022-02-14 19:50:59  143377383 risk/dataset_credit_risk.csv\n",
      "2022-02-14 15:45:10          0 risk/train/_SUCCESS\n",
      "2022-02-14 15:45:09   16075032 risk/train/part-00000-4f4645d1-cb41-4959-8e10-bed4fdbabb6f-c000.csv\n",
      "2022-02-14 03:29:11   20375922 risk/train_model.csv\n",
      "2022-02-14 18:06:42       1131 script-risk-eval-2022-02-14-18-06-40-922/input/code/evaluation.py\n",
      "2022-02-14 18:06:42       1131 script-risk-eval-2022-02-14-18-06-41-490/input/code/evaluation.py\n",
      "2022-02-14 18:06:42       1131 script-risk-eval-2022-02-14-18-06-41-837/input/code/evaluation.py\n",
      "2022-02-14 18:08:03       1131 script-risk-eval-2022-02-14-18-08-02-598/input/code/evaluation.py\n",
      "2022-02-14 18:08:04       1131 script-risk-eval-2022-02-14-18-08-03-223/input/code/evaluation.py\n",
      "2022-02-14 18:09:26       1131 script-risk-eval-2022-02-14-18-09-25-382/input/code/evaluation.py\n",
      "2022-02-14 18:09:58       1131 script-risk-eval-2022-02-14-18-09-57-748/input/code/evaluation.py\n",
      "2022-02-14 18:12:23       1131 script-risk-eval-2022-02-14-18-12-22-130/input/code/evaluation.py\n",
      "2022-02-14 18:13:09       1131 script-risk-eval-2022-02-14-18-13-08-768/input/code/evaluation.py\n",
      "2022-02-14 18:13:10       1131 script-risk-eval-2022-02-14-18-13-09-365/input/code/evaluation.py\n",
      "2022-02-14 18:13:10       1131 script-risk-eval-2022-02-14-18-13-09-924/input/code/evaluation.py\n",
      "2022-02-14 19:39:47       1131 script-risk-eval-2022-02-14-19-39-46-696/input/code/evaluation.py\n",
      "2022-02-14 19:39:48       1131 script-risk-eval-2022-02-14-19-39-47-322/input/code/evaluation.py\n",
      "2022-02-14 19:39:48       1131 script-risk-eval-2022-02-14-19-39-47-866/input/code/evaluation.py\n",
      "2022-02-14 19:51:01       1131 script-risk-eval-2022-02-14-19-51-00-820/input/code/evaluation.py\n",
      "2022-02-14 19:51:02       1131 script-risk-eval-2022-02-14-19-51-01-676/input/code/evaluation.py\n",
      "2022-02-14 19:51:03       1131 script-risk-eval-2022-02-14-19-51-02-135/input/code/evaluation.py\n",
      "2022-02-14 05:07:48       2051 sm-spark-2022-02-14-05-07-47-781/input/code/preprocessing.py\n",
      "2022-02-14 05:36:25       1853 sm-spark-2022-02-14-05-36-24-331/input/code/preprocessing.py\n",
      "2022-02-14 05:36:25       1853 sm-spark-2022-02-14-05-36-24-848/input/code/preprocessing.py\n",
      "2022-02-14 06:14:07       1848 sm-spark-2022-02-14-06-14-06-335/input/code/preprocessing.py\n",
      "2022-02-14 06:14:07       1848 sm-spark-2022-02-14-06-14-06-758/input/code/preprocessing.py\n",
      "2022-02-14 06:17:17       1848 sm-spark-2022-02-14-06-17-16-397/input/code/preprocessing.py\n",
      "2022-02-14 06:17:17       1848 sm-spark-2022-02-14-06-17-16-804/input/code/preprocessing.py\n",
      "2022-02-14 06:26:20       1725 sm-spark-2022-02-14-06-26-19-866/input/code/preprocessing.py\n",
      "2022-02-14 06:26:21       1725 sm-spark-2022-02-14-06-26-20-301/input/code/preprocessing.py\n",
      "2022-02-14 06:37:47       1674 sm-spark-2022-02-14-06-37-46-438/input/code/preprocessing.py\n",
      "2022-02-14 06:37:48       1674 sm-spark-2022-02-14-06-37-46-981/input/code/preprocessing.py\n",
      "2022-02-14 06:50:27       1644 sm-spark-2022-02-14-06-50-26-660/input/code/preprocessing.py\n",
      "2022-02-14 06:50:28       1644 sm-spark-2022-02-14-06-50-27-098/input/code/preprocessing.py\n",
      "2022-02-14 07:01:47       1655 sm-spark-2022-02-14-07-01-46-421/input/code/preprocessing.py\n",
      "2022-02-14 07:01:47       1655 sm-spark-2022-02-14-07-01-46-954/input/code/preprocessing.py\n",
      "2022-02-14 07:34:38       1655 sm-spark-2022-02-14-07-34-37-738/input/code/preprocessing.py\n",
      "2022-02-14 07:34:39       1655 sm-spark-2022-02-14-07-34-38-168/input/code/preprocessing.py\n",
      "2022-02-14 07:56:58       1680 sm-spark-2022-02-14-07-56-57-399/input/code/preprocessing.py\n",
      "2022-02-14 07:56:58       1680 sm-spark-2022-02-14-07-56-57-856/input/code/preprocessing.py\n",
      "2022-02-14 08:21:47       1680 sm-spark-2022-02-14-08-21-46-760/input/code/preprocessing.py\n",
      "2022-02-14 08:21:48       1680 sm-spark-2022-02-14-08-21-47-076/input/code/preprocessing.py\n",
      "2022-02-14 08:30:23       1674 sm-spark-2022-02-14-08-30-22-400/input/code/preprocessing.py\n",
      "2022-02-14 08:30:23       1674 sm-spark-2022-02-14-08-30-22-944/input/code/preprocessing.py\n",
      "2022-02-14 15:07:32       2791 sm-spark-2022-02-14-15-07-31-753/input/code/preprocessing.py\n",
      "2022-02-14 15:07:33       2791 sm-spark-2022-02-14-15-07-32-323/input/code/preprocessing.py\n",
      "2022-02-14 15:19:48       2791 sm-spark-2022-02-14-15-19-46-905/input/code/preprocessing.py\n",
      "2022-02-14 15:19:48       2791 sm-spark-2022-02-14-15-19-47-431/input/code/preprocessing.py\n",
      "2022-02-14 15:26:31       2777 sm-spark-2022-02-14-15-26-30-346/input/code/preprocessing.py\n",
      "2022-02-14 15:26:31       2777 sm-spark-2022-02-14-15-26-30-826/input/code/preprocessing.py\n",
      "2022-02-14 15:40:03       2469 sm-spark-2022-02-14-15-40-02-434/input/code/preprocessing.py\n",
      "2022-02-14 15:40:03       2469 sm-spark-2022-02-14-15-40-02-893/input/code/preprocessing.py\n",
      "2022-02-14 18:06:42       2164 sm-spark-2022-02-14-18-06-41-285/input/code/preprocessing.py\n",
      "2022-02-14 18:06:42       2164 sm-spark-2022-02-14-18-06-41-596/input/code/preprocessing.py\n",
      "2022-02-14 18:08:04       2164 sm-spark-2022-02-14-18-08-03-034/input/code/preprocessing.py\n",
      "2022-02-14 18:09:26       2164 sm-spark-2022-02-14-18-09-25-300/input/code/preprocessing.py\n",
      "2022-02-14 18:09:58       2164 sm-spark-2022-02-14-18-09-57-658/input/code/preprocessing.py\n",
      "2022-02-14 18:10:21       2164 sm-spark-2022-02-14-18-10-20-350/input/code/preprocessing.py\n",
      "2022-02-14 18:12:23       2164 sm-spark-2022-02-14-18-12-22-053/input/code/preprocessing.py\n",
      "2022-02-14 18:13:10       2164 sm-spark-2022-02-14-18-13-09-185/input/code/preprocessing.py\n",
      "2022-02-14 18:13:10       2164 sm-spark-2022-02-14-18-13-09-695/input/code/preprocessing.py\n",
      "2022-02-14 19:39:48       2164 sm-spark-2022-02-14-19-39-47-121/input/code/preprocessing.py\n",
      "2022-02-14 19:39:48       2164 sm-spark-2022-02-14-19-39-47-695/input/code/preprocessing.py\n",
      "2022-02-14 19:51:02       2164 sm-spark-2022-02-14-19-51-01-268/input/code/preprocessing.py\n",
      "2022-02-14 19:51:03       2164 sm-spark-2022-02-14-19-51-02-049/input/code/preprocessing.py\n",
      "2022-02-14 20:05:27       2164 sm-spark-2022-02-14-20-05-26-757/input/code/preprocessing.py\n",
      "2022-02-14 20:05:28       2164 sm-spark-2022-02-14-20-05-27-151/input/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! aws s3 ls sagemaker-us-east-2-690391346049 --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
